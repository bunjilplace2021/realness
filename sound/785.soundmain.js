(self.webpackChunkrealness=self.webpackChunkrealness||[]).push([[785],{785:(__unused_webpack___webpack_module__,__webpack_exports__,__webpack_require__)=>{"use strict";eval("// ESM COMPAT FLAG\n__webpack_require__.r(__webpack_exports__);\n\n// EXPORTS\n__webpack_require__.d(__webpack_exports__, {\n  \"default\": () => /* binding */ audio_recorder_polyfill\n});\n\n// CONCATENATED MODULE: ./node_modules/audio-recorder-polyfill/wave-encoder/index.js\n// Copied from https://github.com/chris-rudmin/Recorderjs\n\n/* harmony default export */ const wave_encoder = (() => {\n  let BYTES_PER_SAMPLE = 2\n\n  let recorded = []\n\n  function encode (buffer) {\n    let length = buffer.length\n    let data = new Uint8Array(length * BYTES_PER_SAMPLE)\n    for (let i = 0; i < length; i++) {\n      let index = i * BYTES_PER_SAMPLE\n      let sample = buffer[i]\n      if (sample > 1) {\n        sample = 1\n      } else if (sample < -1) {\n        sample = -1\n      }\n      sample = sample * 32768\n      data[index] = sample\n      data[index + 1] = sample >> 8\n    }\n    recorded.push(data)\n  }\n\n  function dump (sampleRate) {\n    let bufferLength = recorded.length ? recorded[0].length : 0\n    let length = recorded.length * bufferLength\n    let wav = new Uint8Array(44 + length)\n    let view = new DataView(wav.buffer)\n\n    // RIFF identifier 'RIFF'\n    view.setUint32(0, 1380533830, false)\n    // file length minus RIFF identifier length and file description length\n    view.setUint32(4, 36 + length, true)\n    // RIFF type 'WAVE'\n    view.setUint32(8, 1463899717, false)\n    // format chunk identifier 'fmt '\n    view.setUint32(12, 1718449184, false)\n    // format chunk length\n    view.setUint32(16, 16, true)\n    // sample format (raw)\n    view.setUint16(20, 1, true)\n    // channel count\n    view.setUint16(22, 1, true)\n    // sample rate\n    view.setUint32(24, sampleRate, true)\n    // byte rate (sample rate * block align)\n    view.setUint32(28, sampleRate * BYTES_PER_SAMPLE, true)\n    // block align (channel count * bytes per sample)\n    view.setUint16(32, BYTES_PER_SAMPLE, true)\n    // bits per sample\n    view.setUint16(34, 8 * BYTES_PER_SAMPLE, true)\n    // data chunk identifier 'data'\n    view.setUint32(36, 1684108385, false)\n    // data chunk length\n    view.setUint32(40, length, true)\n\n    // eslint-disable-next-line unicorn/no-for-loop\n    for (let i = 0; i < recorded.length; i++) {\n      wav.set(recorded[i], i * bufferLength + 44)\n    }\n\n    recorded = []\n    postMessage(wav.buffer, [wav.buffer])\n  }\n\n  onmessage = e => {\n    if (e.data[0] === 'encode') {\n      encode(e.data[1])\n    } else if (e.data[0] === 'dump') {\n      dump(e.data[1])\n    }\n  }\n});\n\n// CONCATENATED MODULE: ./node_modules/audio-recorder-polyfill/index.js\n;\n\nlet AudioContext = window.AudioContext || window.webkitAudioContext\n\nlet createWorker = fn => {\n  let js = fn\n    .toString()\n    .replace(/^(\\(\\)\\s*=>|function\\s*\\(\\))\\s*{/, '')\n    .replace(/}$/, '')\n  let blob = new Blob([js])\n  return new Worker(URL.createObjectURL(blob))\n}\n\nlet error = method => {\n  let event = new Event('error')\n  event.data = new Error('Wrong state for ' + method)\n  return event\n}\n\nlet context\n\n/**\n * Audio Recorder with MediaRecorder API.\n *\n * @example\n * navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {\n *   let recorder = new MediaRecorder(stream)\n * })\n */\nclass MediaRecorder {\n  /**\n   * @param {MediaStream} stream The audio stream to record.\n   */\n  constructor (stream, config = null) {\n    /**\n     * The `MediaStream` passed into the constructor.\n     * @type {MediaStream}\n     */\n    this.stream = stream\n    this.config = config\n    /**\n     * The current state of recording process.\n     * @type {\"inactive\"|\"recording\"|\"paused\"}\n     */\n    this.state = 'inactive'\n\n    this.em = document.createDocumentFragment()\n    this.encoder = createWorker(MediaRecorder.encoder)\n\n    let recorder = this\n    this.encoder.addEventListener('message', e => {\n      let event = new Event('dataavailable')\n      event.data = new Blob([e.data], { type: recorder.mimeType })\n      recorder.em.dispatchEvent(event)\n      if (recorder.state === 'inactive') {\n        recorder.em.dispatchEvent(new Event('stop'))\n      }\n    })\n  }\n\n  /**\n   * Begins recording media.\n   *\n   * @param {number} [timeslice] The milliseconds to record into each `Blob`.\n   *                             If this parameter isnâ€™t included, single `Blob`\n   *                             will be recorded.\n   *\n   * @return {undefined}\n   *\n   * @example\n   * recordButton.addEventListener('click', () => {\n   *   recorder.start()\n   * })\n   */\n  start (timeslice) {\n    if (this.state !== 'inactive') {\n      return this.em.dispatchEvent(error('start'))\n    }\n\n    this.state = 'recording'\n\n    if (!context) {\n      context = new AudioContext(this.config)\n    }\n    this.clone = this.stream.clone()\n    this.input = context.createMediaStreamSource(this.clone)\n    this.processor = context.createScriptProcessor(2048, 1, 1)\n\n    this.encoder.postMessage(['init', context.sampleRate])\n\n    this.processor.onaudioprocess = e => {\n      if (this.state === 'recording') {\n        this.encoder.postMessage(['encode', e.inputBuffer.getChannelData(0)])\n      }\n    }\n\n    this.input.connect(this.processor)\n    this.processor.connect(context.destination)\n\n    this.em.dispatchEvent(new Event('start'))\n\n    if (timeslice) {\n      this.slicing = setInterval(() => {\n        if (this.state === 'recording') this.requestData()\n      }, timeslice)\n    }\n\n    return undefined\n  }\n\n  /**\n   * Stop media capture and raise `dataavailable` event with recorded data.\n   *\n   * @return {undefined}\n   *\n   * @example\n   * finishButton.addEventListener('click', () => {\n   *   recorder.stop()\n   * })\n   */\n  stop () {\n    if (this.state === 'inactive') {\n      return this.em.dispatchEvent(error('stop'))\n    }\n\n    this.requestData()\n    this.state = 'inactive'\n    this.clone.getTracks().forEach(track => {\n      track.stop()\n    })\n    this.processor.disconnect()\n    this.input.disconnect()\n    return clearInterval(this.slicing)\n  }\n\n  /**\n   * Pauses recording of media streams.\n   *\n   * @return {undefined}\n   *\n   * @example\n   * pauseButton.addEventListener('click', () => {\n   *   recorder.pause()\n   * })\n   */\n  pause () {\n    if (this.state !== 'recording') {\n      return this.em.dispatchEvent(error('pause'))\n    }\n\n    this.state = 'paused'\n    return this.em.dispatchEvent(new Event('pause'))\n  }\n\n  /**\n   * Resumes media recording when it has been previously paused.\n   *\n   * @return {undefined}\n   *\n   * @example\n   * resumeButton.addEventListener('click', () => {\n   *   recorder.resume()\n   * })\n   */\n  resume () {\n    if (this.state !== 'paused') {\n      return this.em.dispatchEvent(error('resume'))\n    }\n\n    this.state = 'recording'\n    return this.em.dispatchEvent(new Event('resume'))\n  }\n\n  /**\n   * Raise a `dataavailable` event containing the captured media.\n   *\n   * @return {undefined}\n   *\n   * @example\n   * this.on('nextData', () => {\n   *   recorder.requestData()\n   * })\n   */\n  requestData () {\n    if (this.state === 'inactive') {\n      return this.em.dispatchEvent(error('requestData'))\n    }\n\n    return this.encoder.postMessage(['dump', context.sampleRate])\n  }\n\n  /**\n   * Add listener for specified event type.\n   *\n   * @param {\"start\"|\"stop\"|\"pause\"|\"resume\"|\"dataavailable\"|\"error\"}\n   * type Event type.\n   * @param {function} listener The listener function.\n   *\n   * @return {undefined}\n   *\n   * @example\n   * recorder.addEventListener('dataavailable', e => {\n   *   audio.src = URL.createObjectURL(e.data)\n   * })\n   */\n  addEventListener (...args) {\n    this.em.addEventListener(...args)\n  }\n\n  /**\n   * Remove event listener.\n   *\n   * @param {\"start\"|\"stop\"|\"pause\"|\"resume\"|\"dataavailable\"|\"error\"}\n   * type Event type.\n   * @param {function} listener The same function used in `addEventListener`.\n   *\n   * @return {undefined}\n   */\n  removeEventListener (...args) {\n    this.em.removeEventListener(...args)\n  }\n\n  /**\n   * Calls each of the listeners registered for a given event.\n   *\n   * @param {Event} event The event object.\n   *\n   * @return {boolean} Is event was no canceled by any listener.\n   */\n  dispatchEvent (...args) {\n    this.em.dispatchEvent(...args)\n  }\n}\n\n/**\n * The MIME type that is being used for recording.\n * @type {string}\n */\nMediaRecorder.prototype.mimeType = 'audio/wav'\n\n/**\n * Returns `true` if the MIME type specified is one the polyfill can record.\n *\n * This polyfill supports `audio/wav` and `audio/mpeg`.\n *\n * @param {string} mimeType The mimeType to check.\n *\n * @return {boolean} `true` on `audio/wav` and `audio/mpeg` MIME type.\n */\nMediaRecorder.isTypeSupported = mimeType => {\n  return MediaRecorder.prototype.mimeType === mimeType\n}\n\n/**\n * `true` if MediaRecorder can not be polyfilled in the current browser.\n * @type {boolean}\n *\n * @example\n * if (MediaRecorder.notSupported) {\n *   showWarning('Audio recording is not supported in this browser')\n * }\n */\nMediaRecorder.notSupported = !navigator.mediaDevices || !AudioContext\n\n/**\n * Converts RAW audio buffer to compressed audio files.\n * It will be loaded to Web Worker.\n * By default, WAVE encoder will be used.\n * @type {function}\n *\n * @example\n * MediaRecorder.prototype.mimeType = 'audio/ogg'\n * MediaRecorder.encoder = oggEncoder\n */\nMediaRecorder.encoder = wave_encoder\n\n/* harmony default export */ const audio_recorder_polyfill = (MediaRecorder);\n\n\n//# sourceURL=webpack://realness/./node_modules/audio-recorder-polyfill/index.js_+_1_modules?")}}]);